{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef4816f-64fc-4947-a3cd-b87fce80bec5",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05765480-adde-40a5-9a4a-84f5cc59d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Overfitting and underfitting are two common problems in machine learning:\n",
    "\n",
    "Overfitting occurs when a machine learning model learns the training data too well, capturing noise and random \n",
    "fluctuations in the data rather than the underlying patterns. This results in a model that performs extremely well on \n",
    "the training data but poorly on new, unseen data.\n",
    "\n",
    "1.Consequences of overfitting:\n",
    "\n",
    "    ~Poor generalization: The model fails to generalize to new data.\n",
    "    ~High variance: The model is highly sensitive to variations in the training data.\n",
    "    ~Memorization: The model may memorize the training data instead of learning meaningful patterns.\n",
    "    \n",
    "Mitigation strategies for overfitting:\n",
    "\n",
    "    ~Regularization: Add penalties to the model's complexity to discourage it from fitting noise in the data.\n",
    "    ~Cross-validation: Use techniques like k-fold cross-validation to assess model performance on multiple subsets of\n",
    "     the data.\n",
    "    ~Feature selection: Remove irrelevant or redundant features from the dataset.\n",
    "    ~More data: Increasing the size of the training dataset can help the model learn genuine patterns.\n",
    "    ~Simpler model: Choose a simpler model architecture with fewer parameters.\n",
    "    ~Underfitting occurs when a machine learning model is too simple to capture the underlying patterns in the data. \n",
    "     The model performs poorly on both the training data and new data.\n",
    "\n",
    "2.Consequences of underfitting:\n",
    "\n",
    "    ~Poor model performance: The model fails to capture important relationships in the data.\n",
    "    ~High bias: The model is too simplistic and cannot represent complex data.\n",
    "    \n",
    "Mitigation strategies for underfitting:\n",
    "\n",
    "    ~Complexity increase: Use a more complex model architecture or algorithm that can better represent the data.\n",
    "    ~Feature engineering: Create new features or transform existing ones to help the model capture patterns.\n",
    "    ~Fine-tuning hyperparameters: Adjust hyperparameters like learning rate, regularization strength, or tree depth to \n",
    "     improve model performance.\n",
    "    ~More data: Increasing the size of the training dataset can help a more complex model generalize better.\n",
    "    \n",
    "Finding the right balance between model complexity and data fitting is crucial to address both overfitting and \n",
    "underfitting. Techniques like cross-validation and monitoring learning curves can help practitioners diagnose and\n",
    "mitigate these issues effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaae626-df00-49bc-9abf-c96ae944ebf1",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e192fa-d6df-4293-85b4-896b5269ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reducing overfitting in machine learning involves implementing strategies to prevent a model from fitting noise in the\n",
    "training data and promoting better generalization to new, unseen data. Here are some common techniques to reduce\n",
    "overfitting:\n",
    "\n",
    "1.Regularization: Regularization techniques add penalties to the model's complexity, discouraging it from fitting noise.\n",
    "  Two common types of regularization are L1 (Lasso) and L2 (Ridge) regularization, which add penalty terms to the loss \n",
    "function, influencing the magnitude of the model's coefficients.\n",
    "\n",
    "2.Cross-Validation: Use techniques like k-fold cross-validation to assess model performance on multiple subsets of the\n",
    "  data. Cross-validation helps in estimating how well the model will generalize to unseen data and provides a more\n",
    "robust evaluation of its performance.\n",
    "\n",
    "3.Feature Selection: Remove irrelevant or redundant features from the dataset. Feature selection focuses on retaining \n",
    "  only the most informative and meaningful attributes, which can lead to a simpler and more robust model.\n",
    "\n",
    "4.Early Stopping: Monitor the model's performance on a validation set during training. Stop training when the validation\n",
    "  performance starts to degrade. This prevents the model from continuing to overfit the training data.\n",
    "\n",
    "5.Ensemble Methods: Ensemble methods, such as Random Forests and Gradient Boosting, combine multiple models to improve\n",
    "  predictive performance and reduce overfitting. By averaging or combining the predictions of multiple models, ensembles \n",
    "can capture more robust patterns in the data.\n",
    "\n",
    "6.Simpler Model Architecture: Choose a simpler model architecture with fewer parameters. Sometimes, complex models with\n",
    "  a large number of parameters are more prone to overfitting. Starting with a simpler model and gradually increasing \n",
    "complexity can help find the right balance.\n",
    "\n",
    "7.Data Augmentation: Increase the effective size of the training dataset by applying data augmentation techniques. This\n",
    "  involves creating new training examples by applying random transformations to existing data, which can help the model\n",
    "generalize better.\n",
    "\n",
    "8.More Data: Increasing the size of the training dataset can often reduce overfitting. A larger dataset provides the\n",
    "  model with more diverse examples, making it harder to memorize the training data.\n",
    "\n",
    "9.Dropout: In neural networks, dropout is a regularization technique where random neurons are temporarily dropped out\n",
    "  (set to zero) during training. This prevents co-adaptation of neurons and helps prevent overfitting.\n",
    "\n",
    "10.Parameter Tuning: Fine-tune hyperparameters like learning rate, batch size, and regularization strength to optimize\n",
    "   the model's performance. Grid search or randomized search can be used to find the best hyperparameter values.\n",
    "\n",
    "The choice of which technique(s) to use depends on the specific problem, dataset, and model being employed. Often, a\n",
    "combination of these techniques is used to effectively reduce overfitting and improve the generalization performance of\n",
    "a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a3062-421d-4d02-97e1-8fae7078ccaa",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14589fa-2a0f-48b9-b461-55f1fcfaab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Underfitting in machine learning occurs when a model is too simplistic to capture the underlying patterns or\n",
    "relationships in the data. It typically results in poor performance, both on the training data and on new, unseen data.\n",
    "Underfit models are unable to learn the complexities of the data and make overly simplistic assumptions. Here are some\n",
    "scenarios where underfitting can occur in machine learning:\n",
    "\n",
    "1.Model Complexity is Too Low: If you choose a model that is too simple for the complexity of the data, it may underfit.\n",
    "  For example, using a linear regression model to fit highly non-linear data.\n",
    "\n",
    "2.Insufficient Features: If you haven't included enough relevant features or have removed important ones during feature\n",
    "  selection, the model may lack the necessary information to represent the data adequately.\n",
    "\n",
    "3.Over-Regularization: Excessive regularization, such as a very high penalty in L1 or L2 regularization, can constrain\n",
    "  the model too much, making it overly simplistic.\n",
    "\n",
    "4.Small Training Dataset: When the training dataset is small, it may not provide enough examples for the model to learn\n",
    "  from. This can lead to underfitting, especially for complex problems.\n",
    "\n",
    "5.High Bias Algorithms: Certain algorithms have inherent bias, and if they are used in situations where more flexible\n",
    "  models are needed, they may underfit the data. For instance, using a decision tree with a shallow depth on complex\n",
    "data.\n",
    "\n",
    "6.Data Scaling Issues: In some cases, not scaling or normalizing the data properly can lead to underfitting. For\n",
    "  algorithms that are sensitive to feature scales (e.g., k-nearest neighbors), this can be a problem.\n",
    "\n",
    "7.Improper Handling of Categorical Variables: If categorical variables are not properly encoded or handled in a way that\n",
    "  loses important information, the model may underfit.\n",
    "\n",
    "8.Ignoring Outliers: If outliers are present in the data and not appropriately treated, they can lead to underfitting,\n",
    "  as the model may attempt to fit the outliers at the expense of the majority of the data.\n",
    "\n",
    "9.Overly Simplistic Assumptions: Sometimes, domain-specific assumptions or constraints are overly simplistic and do not\n",
    "  capture the true complexity of the problem. If such assumptions are imposed on the model, it may underfit.\n",
    "\n",
    "10.Early Stopping: In deep learning, if early stopping is used too aggressively, the model might not reach its optimal\n",
    "   performance, resulting in underfitting.\n",
    "\n",
    "11.Ignoring Temporal Dynamics: In time-series data, ignoring temporal dependencies or seasonality patterns can lead to\n",
    "   underfitting.\n",
    "\n",
    "To mitigate underfitting, it's essential to choose appropriate models, ensure adequate feature engineering, and avoid\n",
    "overly simplistic assumptions. Increasing model complexity, adding more features, using larger datasets, and fine-tuning \n",
    "hyperparameters are some of the strategies that can help address underfitting and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb80c56-5218-4c31-a49b-b375f857b6aa",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66a3fc-5ea2-4d9a-aa17-d92e4095eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the balance between two sources\n",
    "of errors that affect a model's predictive performance: bias and variance. Understanding this tradeoff is crucial for\n",
    "building models that generalize well to unseen data.\n",
    "\n",
    "Here's an explanation of bias and variance and their relationship:\n",
    "\n",
    "1.Bias:\n",
    "\n",
    "    ~Definition: Bias refers to the error introduced by overly simplistic assumptions in the learning algorithm. A model\n",
    "     with high bias tends to underfit the data.\n",
    "    ~Characteristics: High bias models are too simplistic and unable to capture complex patterns in the data. They make\n",
    "     strong assumptions about the underlying relationships between features and the target variable, which may not hold\n",
    "    in reality.\n",
    "    ~Effects on Model Performance: High bias results in poor performance on both the training data and new, unseen data.\n",
    "     The model systematically misses the true relationships in the data.\n",
    "        \n",
    "2.Variance:\n",
    "\n",
    "    ~Definition: Variance refers to the error introduced by the model's sensitivity to fluctuations in the training\n",
    "     data. A model with high variance tends to overfit the data.\n",
    "    ~Characteristics: High variance models are overly complex and capture noise or random fluctuations in the training\n",
    "     data. They adapt too closely to the training data and may not generalize well to new data.\n",
    "    ~Effects on Model Performance: High variance leads to excellent performance on the training data but poor \n",
    "     performance on new, unseen data. The model is essentially memorizing the training data rather than learning\n",
    "    meaningful patterns.\n",
    "    \n",
    "Relationship between Bias and Variance:\n",
    "\n",
    "    ~As the complexity of a model increases, bias tends to decrease, but variance tends to increase. This is because \n",
    "     more complex models can fit the training data more closely.\n",
    "    ~Conversely, as the complexity of a model decreases (e.g., using simpler models), bias tends to increase, but\n",
    "     variance decreases.\n",
    "        \n",
    "The Tradeoff:\n",
    "\n",
    "    ~The goal in machine learning is to strike a balance between bias and variance to achieve good model generalization.\n",
    "    ~There is usually an optimal level of model complexity that minimizes the expected prediction error (a combination \n",
    "     of bias and variance) on new, unseen data. This optimal complexity depends on the specific problem and dataset.\n",
    "    ~The tradeoff implies that reducing bias might increase variance, and vice versa. Therefore, model selection and \n",
    "      tuning involve finding the right level of complexity for a given problem.\n",
    "        \n",
    "Impact on Model Performance:\n",
    "\n",
    "    ~Underfit models (high bias, low variance) perform poorly on both training and test data.\n",
    "    ~Overfit models (low bias, high variance) perform well on training data but poorly on test data.\n",
    "    ~The ideal model (appropriate bias-variance tradeoff) generalizes well to both training and test data, providing\n",
    "      good predictive performance.\n",
    "        \n",
    "In summary, the bias-variance tradeoff highlights the importance of finding the right level of model complexity to\n",
    "achieve the best tradeoff between bias and variance. This balance helps create models that generalize effectively to \n",
    "new, unseen data and make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a573c2c-1f28-4c75-82b8-0150fab545be",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a654838-ab48-4b04-adc7-e8d710b82c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detecting overfitting and underfitting in machine learning models is crucial for assessing model performance and making\n",
    "necessary adjustments to improve generalization. Here are common methods for detecting these issues and determining\n",
    "whether your model is overfitting or underfitting:\n",
    "\n",
    "Methods for Detecting Overfitting:\n",
    "\n",
    "1.Validation Curves: Plot the training and validation performance (e.g., accuracy or error) as a function of a\n",
    "  hyperparameter, such as model complexity or regularization strength. Overfitting is indicated when the training \n",
    "performance continues to improve while the validation performance starts to degrade.\n",
    "\n",
    "2.Learning Curves: Plot the training and validation performance as a function of the number of training examples. In\n",
    "  overfit models, the training performance may approach perfect accuracy, but the validation performance remains poor\n",
    "or plateaus.\n",
    "\n",
    "3.Cross-Validation: Use k-fold cross-validation to assess model performance on multiple subsets of the data. Overfit\n",
    "  models tend to have high variance in their performance across different folds, indicating sensitivity to the specific\n",
    "training data.\n",
    "\n",
    "4.Regularization Strength: Experiment with different values of regularization hyperparameters (e.g., L1 or L2 \n",
    "  regularization strength). Overfit models often have smaller regularization values or no regularization at all, leading \n",
    "to large model coefficients.\n",
    "\n",
    "5.Feature Importance Analysis: Analyze feature importance scores to identify features that disproportionately influence\n",
    "  the model's predictions. Overfit models may assign high importance to irrelevant features or noise.\n",
    "\n",
    "Methods for Detecting Underfitting:\n",
    "\n",
    "1.Validation Curves: In cases of underfitting, both the training and validation performance may be poor and show little\n",
    "  improvement as model complexity or hyperparameters change. There is no significant gap between training and validation\n",
    "performance.\n",
    "\n",
    "2.Learning Curves: Learning curves for underfit models show low performance on both training and validation data, and\n",
    "  performance may not improve even with additional training examples.\n",
    "\n",
    "3.Cross-Validation: In underfit models, the performance across different cross-validation folds is consistently poor,\n",
    "  indicating that the model is not capturing essential patterns in the data.\n",
    "\n",
    "4.Model Complexity: Assess the model's complexity relative to the problem. If you suspect underfitting, consider whether\n",
    "  a more complex model architecture or algorithm is needed.\n",
    "\n",
    "5.Feature Engineering: Examine the features used in the model. Underfitting may occur if essential features are missing \n",
    "  or if feature engineering is inadequate.\n",
    "\n",
    "6.Hyperparameter Tuning: Experiment with different hyperparameter values and ensure that they are appropriately tuned.\n",
    "  Increasing model complexity or adjusting other hyperparameters may be necessary to reduce underfitting.\n",
    "\n",
    "7.Visual Inspection: Visualize the model's predictions compared to the true target values. In underfit models,\n",
    "  predictions may exhibit a clear bias or systematic error that is not present in the data.\n",
    "\n",
    "In summary, detecting overfitting and underfitting involves analyzing the performance metrics, learning curves, and\n",
    "other diagnostic tools to assess how well the model generalizes to new data. The key is to strike a balance between bias \n",
    "and variance to achieve optimal model performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2e472-ef7d-44f8-9798-df64bb05b022",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26bf44a-fde4-4fed-ba43-0033255f3b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bias and variance are two critical concepts in machine learning that describe different types of errors in model\n",
    "predictions. Let's compare and contrast bias and variance and provide examples of high bias and high variance models:\n",
    "\n",
    "Bias:\n",
    "\n",
    "    ~Definition: Bias refers to the error introduced by overly simplistic assumptions in the learning algorithm. High \n",
    "     bias models tend to underfit the data.\n",
    "        \n",
    "Characteristics:\n",
    "    ~High bias models are too simplistic and unable to capture complex patterns in the data.\n",
    "    ~They make strong assumptions about the underlying relationships between features and the target variable, which \n",
    "      may not hold in reality.\n",
    "        \n",
    "Effects on Model Performance:\n",
    "    ~High bias results in poor performance on both the training data and new, unseen data.\n",
    "    ~The model systematically misses the true relationships in the data.\n",
    "Examples:\n",
    "    ~Linear regression with insufficient features for a complex problem.\n",
    "    ~A shallow decision tree on a dataset with intricate patterns.\n",
    "    \n",
    "Variance:\n",
    "\n",
    "    ~Definition: Variance refers to the error introduced by the model's sensitivity to fluctuations in the training\n",
    "     data. High variance models tend to overfit the data.\n",
    "        \n",
    "Characteristics:\n",
    "    ~High variance models are overly complex and capture noise or random fluctuations in the training data.\n",
    "    ~They adapt too closely to the training data and may not generalize well to new data.\n",
    "    \n",
    "Effects on Model Performance:\n",
    "    ~High variance leads to excellent performance on the training data but poor performance on new, unseen data.\n",
    "    ~The model is essentially memorizing the training data rather than learning meaningful patterns.\n",
    "    \n",
    "Examples:\n",
    "    ~A deep neural network with too many layers for a small dataset.\n",
    "    ~A decision tree with a very high depth that fits noise in the data.\n",
    "    \n",
    "Comparison:\n",
    "\n",
    "1.Performance on Training Data:\n",
    "\n",
    "    ~High bias models perform poorly on training data (low training accuracy).\n",
    "    ~High variance models perform well on training data (high training accuracy).\n",
    "2.Performance on New Data (Generalization):\n",
    "\n",
    "    ~High bias models perform poorly on new, unseen data (low test accuracy).\n",
    "    ~High variance models perform poorly on new, unseen data (low test accuracy).\n",
    "3.Sensitivity to Data:\n",
    "\n",
    "    ~High bias models are less sensitive to variations in the training data.\n",
    "    ~High variance models are highly sensitive to variations in the training data.\n",
    "4.Complexity:\n",
    "\n",
    "    ~High bias models are typically simple and have fewer parameters.\n",
    "    ~High variance models are often complex and have more parameters.\n",
    "5.Tradeoff:\n",
    "\n",
    "    ~The bias-variance tradeoff highlights that there is usually an optimal level of model complexity that minimizes \n",
    "     the expected prediction error on new data. This balance depends on the specific problem and dataset.\n",
    "        \n",
    "In practice, finding the right balance between bias and variance is crucial. Models that strike this balance generalize\n",
    "effectively to new data, make accurate predictions, and are considered well-fitted to the problem. It's essential to \n",
    "choose appropriate model architectures, adjust hyperparameters, and consider techniques like regularization and cross-\n",
    "validation to manage bias and variance effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1bbd2b-88f4-4e43-a469-7b2ecbc6b544",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b886fc8b-678a-470c-85cf-59de54b7103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization in machine learning is a set of techniques used to prevent overfitting and improve the generalization\n",
    "performance of models. Overfitting occurs when a model fits the training data too closely, capturing noise and making \n",
    "it perform poorly on new, unseen data. Regularization methods introduce additional constraints or penalties on the model\n",
    "to reduce its complexity, discouraging it from fitting noise and irrelevant details in the data. Here are some common \n",
    "regularization techniques and how they work:\n",
    "\n",
    "1.L1 Regularization (Lasso):\n",
    "\n",
    "    ~How it works: L1 regularization adds a penalty term to the model's loss function that is proportional to the \n",
    "     absolute values of the model's coefficients. It encourages sparsity by driving some coefficients to exactly zero,\n",
    "    effectively performing feature selection.\n",
    "    ~Use cases: L1 regularization is useful when you suspect that only a subset of features is relevant, and you want\n",
    "     to automatically select the most important ones.\n",
    "    ~Benefits: Feature selection, improved model interpretability.\n",
    "    ~Example: Lasso regression.\n",
    "    \n",
    "2.L2 Regularization (Ridge):\n",
    "\n",
    "    ~How it works: L2 regularization adds a penalty term to the model's loss function that is proportional to the square\n",
    "     of the model's coefficients. It discourages large coefficient values, which helps in reducing model complexity.\n",
    "    ~Use cases: L2 regularization is a good choice when you want to prevent individual features from having too much\n",
    "     influence on the model's predictions.\n",
    "    ~Benefits: Reduces the magnitude of coefficients, mitigates multicollinearity.\n",
    "    ~Example: Ridge regression.\n",
    "    \n",
    "3.Elastic Net Regularization:\n",
    "\n",
    "    ~How it works: Elastic Net regularization combines both L1 and L2 penalties in the loss function. It balances the\n",
    "     feature selection capabilities of L1 regularization with the coefficient shrinkage of L2 regularization.\n",
    "    ~Use cases: Elastic Net is useful when you want both feature selection and regularization of coefficient values.\n",
    "    ~Benefits: A compromise between L1 and L2 regularization, suitable for a wide range of scenarios.\n",
    "    ~Example: Elastic Net regression.\n",
    "    \n",
    "4.Dropout (Neural Networks):\n",
    "\n",
    "    ~How it works: Dropout is a regularization technique used in neural networks. During training, randomly selected \n",
    "     neurons are \"dropped out\" or deactivated with a certain probability. This prevents co-adaptation of neurons and \n",
    "    encourages robustness.\n",
    "    ~Use cases: Preventing overfitting in deep neural networks.\n",
    "    ~Benefits: Improves generalization, reduces overfitting.\n",
    "    ~Example: Dropout layers in deep neural networks.\n",
    "    \n",
    "5.Early Stopping:\n",
    "\n",
    "    ~How it works: Early stopping is a simple technique that monitors the model's performance on a validation set \n",
    "     during training. When the validation performance starts to degrade (indicating overfitting), training is stopped.\n",
    "    ~Use cases: Preventing overfitting in iterative algorithms like gradient descent.\n",
    "    ~Benefits: Stops training at the right time to prevent overfitting, saves computational resources.\n",
    "    ~Example: Used in various machine learning algorithms, especially those with many hyperparameters.\n",
    "    \n",
    "6.Cross-Validation:\n",
    "\n",
    "    ~HTMLow it works: Cross-validation is not a regularization technique but a validation method. It helps in selecting\n",
    "     the best regularization parameters or hyperparameters by assessing the model's performance on multiple validation \n",
    "    subsets.\n",
    "    ~Use cases: Determining the optimal level of regularization for a model.\n",
    "    ~Benefits: Ensures that the chosen regularization parameters generalize well to new data.\n",
    "    ~Example: Employed in combination with regularization techniques for model selection.\n",
    "    \n",
    "Regularization techniques are essential tools in preventing overfitting and improving the robustness and generalization\n",
    "capabilities of machine learning models. The choice of regularization method and its hyperparameters should be guided\n",
    "by the specific problem and dataset characteristics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
